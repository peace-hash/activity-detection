{"cells":[{"metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"cell_type":"code","source":"import numpy as np \nimport pandas as pd \nimport os\n\nTRAIN_PATH = '/kaggle/input/datafile/train'\nTEST_PATH = '/kaggle/input/datafile1/test'\nVAL_PATH = '/kaggle/input/datavalidation/val'\n\ntrain_datas = {}\ntest_datas = {}\nval_datas = {}\n\nfor dirname, _, filenames in os.walk(TRAIN_PATH):\n    for filename in filenames:\n        train_datas[filename] = os.path.join(dirname, filename)\n\nfor dirname, _, filenames in os.walk(TEST_PATH):\n    for filename in filenames:\n        test_datas[filename] = os.path.join(dirname, filename)\n        \nfor dirname, _, filenames in os.walk(VAL_PATH):\n    for filename in filenames:\n        val_datas[filename] = os.path.join(dirname, filename)\n\n        \nlen(train_datas), len(test_datas), len(val_datas)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"- 20Hz = 20 readings/seconds.\n- Activity Time = 3 minutes.\n- 180 * 20 = 3600 rows/eachactivity/eachsubject"},{"metadata":{"trusted":true},"cell_type":"code","source":"import numpy as np\n\n\ndef window(width, overlap, max_idx):    \n    start = 0\n    if overlap < 0.0 or overlap >= 1.:\n        raise ValueError(\"overlap needs to be a number between 0 and 1\")\n    while True:\n        end = start + width\n        if end >= max_idx:\n            return None\n        yield start, end\n        start += max(int((1-overlap)*width), 1)\n\n        \ndef window_df(df, width, overlap):\n    windows = window(width, overlap, len(df))\n    for start, end in windows:\n        yield df[start:end]\n\n\ndef standardize(df):    \n    return (df - df.mean()) / df.std()\n\n\ndef zero_cross_rate(series):    \n    zero_cross_count = (np.diff(np.sign(series)) != 0).sum()\n    return zero_cross_count / len(series)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"SAMPLING_RATE = 20 \nDATA_COLS = [\"x\", \"y\", \"z\"] \nTARGET_COL = [\"target\"] \n\nVALID_TARGETS = list(\"abcdefghijklmopqrs\".upper())\nprint(len(VALID_TARGETS))","execution_count":2,"outputs":[{"output_type":"stream","text":"18\n","name":"stdout"}]},{"metadata":{"trusted":true},"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nfrom scipy.stats import kurtosis\nimport os\nimport pickle\nfrom collections import defaultdict\n\nN_SECONDS = 5\nOVERLAP = 0.5\n\ndef generate_features(df):\n\n    master_valid = df[df['target'].isin(VALID_TARGETS)]\n\n    feature_matrix = []\n    \n    for target, df in master_valid.groupby('target'):\n        df = standardize(df[DATA_COLS])\n                \n        grp = defaultdict(list)\n        grp['target'] = target\n        samples = window_df(df,width=N_SECONDS*SAMPLING_RATE,overlap=OVERLAP)\n        \n        for sample in samples:\n            means = sample[DATA_COLS].mean()\n            grp['x_mean'].append(means['x'])\n            grp['y_mean'].append(means['y'])\n            grp['z_mean'].append(means['z'])\n            \n            stds = sample[DATA_COLS].std()\n            grp['x_std'].append(stds['x'])\n            grp['y_std'].append(stds['y'])\n            grp['z_std'].append(stds['z'])\n            \n            grp['x_max_min'].append(max(sample[\"x\"]) - min(sample[\"x\"]))\n            grp['y_max_min'].append(max(sample[\"y\"]) - min(sample[\"y\"]))\n            grp['z_max_min'].append(max(sample[\"z\"]) - min(sample[\"z\"]))\n            \n            corrs = sample[DATA_COLS].corr()\n            grp['xy_corr'].append(corrs.loc['x', 'y'])\n            grp['xz_corr'].append(corrs.loc['x', 'z'])\n            grp['yz_corr'].append(corrs.loc['y', 'z'])\n            \n        \n            rms = np.sqrt(np.mean(np.square(sample[DATA_COLS]), axis=1))\n            grp['rms_mean'].append(rms.mean())\n            grp['rms_std'].append(rms.std())\n\n            grp['x_zcr'].append(zero_cross_rate(sample['x']))\n            grp['y_zcr'].append(zero_cross_rate(sample['y']))\n            grp['z_zcr'].append(zero_cross_rate(sample['z']))\n\n            kurtoses = kurtosis(sample[DATA_COLS])\n            grp['x_kurtosis'].append(kurtoses[0])\n            grp['y_kurtosis'].append(kurtoses[1])\n            grp['z_kurtosis'].append(kurtoses[2])\n\n  \n            x_fft = abs(np.fft.rfft(sample['x']))\n            y_fft = abs(np.fft.rfft(sample['y']))\n            z_fft = abs(np.fft.rfft(sample['z']))\n\n            grp['x_freq_max'].append(np.argmax(x_fft))\n            grp['y_freq_max'].append(np.argmax(y_fft))\n            grp['z_freq_max'].append(np.argmax(z_fft))\n\n    \n            grp['x_fft_max'].append(x_fft.max())\n            grp['y_fft_max'].append(y_fft.max())\n            grp['z_fft_max'].append(z_fft.max())\n            \n \n            grp['x_fft_mean'].append(x_fft.mean())\n            grp['y_fft_mean'].append(y_fft.mean())\n            grp['z_fft_mean'].append(z_fft.mean())\n\n        \n            grp['x_fft_std'].append(x_fft.std())\n            grp['y_fft_std'].append(y_fft.std())\n            grp['z_fft_std'].append(z_fft.std())\n\n            grp['x_fft_kurtosis'].append(kurtosis(x_fft))\n            grp['y_fft_kurtosis'].append(kurtosis(y_fft))\n            grp['z_fft_kurtosis'].append(kurtosis(z_fft))\n\n\n        feature_matrix.append(pd.DataFrame(grp))\n\n \n    feature_matrix_df = pd.concat(feature_matrix)\n    \n    return feature_matrix_df\n","execution_count":1,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def preprocess_data(data_paths):\n    feature_dfs = []\n    for txt_file in data_paths:\n        txt_hints = txt_file.split('_')\n        print(\"Pre-Processing {} ...\".format(data_paths[txt_file]))\n        df = pd.read_csv(data_paths[txt_file],names = ['id', 'target', 'timestamp', 'x', 'y', 'z'])\n        df['z'] = df['z'].apply(lambda x: float(x.strip(';')))\n\n        fdf = generate_features(df)\n        fdf['idx'] = int(txt_hints[1])\n        fdf['sensor'] = txt_hints[2]\n        fdf['device'] = txt_hints[3].split('.')[0]\n        feature_dfs.append(fdf)\n    feature_df = pd.concat(feature_dfs)\n    return feature_df","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_data = preprocess_data(train_datas)\noutput_file = \"train_data.pkl\"\nwith open(output_file, 'wb') as f:\n    pickle.dump(train_data, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"test_data = preprocess_data(test_datas)\n\noutput_file = \"test_data.pkl\"\nwith open(output_file, 'wb') as f:\n    pickle.dump(test_data, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"val_data = preprocess_data(val_datas)\noutput_file = \"val_data.pkl\"\nwith open(output_file, 'wb') as f:\n    pickle.dump(val_data, f)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"!ls","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat":4,"nbformat_minor":4}